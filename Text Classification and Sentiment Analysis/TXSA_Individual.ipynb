{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "described-transcript",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You probably worked out that a backslash means that the following character is deprived of its special powers and must literally match a specific character in the word.', 'Thus, while .', 'is special, \\\\.', 'only matches a period.', 'The braced expressions, like {3,5}, specify the number of repeats of the previous item.', 'The pipe character indicates a choice between the material on its left or its right.', 'Parentheses indicate the scope of an operator: they can be used together with the pipe (or disjunction) symbol like this: «w(i|e|ai|oo)t», matching wit, wet, wait, and woot.', 'It is instructive to see what happens when you omit the parentheses from the last expression above.']\n"
     ]
    }
   ],
   "source": [
    "#Question 1.1\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#read data from Data_1\n",
    "individual_data = open(\"C:/Users/jared/Desktop/Data_1.txt\",\"r\")\n",
    "txt_data = individual_data.read()\n",
    "\n",
    "#tokenize sentence and report output\n",
    "sentence_tokens = nltk.tokenize.sent_tokenize(txt_data)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "metallic-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization using .split function\n",
      "['You', 'probably', 'worked', 'out', 'that', 'a', 'backslash', 'means', 'that', 'the', 'following', 'character', 'is', 'deprived', 'of', 'its', 'special', 'powers', 'and', 'must', 'literally', 'match', 'a', 'specific', 'character', 'in', 'the', 'word.', 'Thus,', 'while', '.', 'is', 'special,', '\\\\.', 'only', 'matches', 'a', 'period.', 'The', 'braced', 'expressions,', 'like', '{3,5},', 'specify', 'the', 'number', 'of', 'repeats', 'of', 'the', 'previous', 'item.', 'The', 'pipe', 'character', 'indicates', 'a', 'choice', 'between', 'the', 'material', 'on', 'its', 'left', 'or', 'its', 'right.', 'Parentheses', 'indicate', 'the', 'scope', 'of', 'an', 'operator:', 'they', 'can', 'be', 'used', 'together', 'with', 'the', 'pipe', '(or', 'disjunction)', 'symbol', 'like', 'this:', '«w(i|e|ai|oo)t»,', 'matching', 'wit,', 'wet,', 'wait,', 'and', 'woot.', 'It', 'is', 'instructive', 'to', 'see', 'what', 'happens', 'when', 'you', 'omit', 'the', 'parentheses', 'from', 'the', 'last', 'expression', 'above.']\n",
      "\n",
      "\n",
      "Tokenization using nltk\n",
      "['You', 'probably', 'worked', 'out', 'that', 'a', 'backslash', 'means', 'that', 'the', 'following', 'character', 'is', 'deprived', 'of', 'its', 'special', 'powers', 'and', 'must', 'literally', 'match', 'a', 'specific', 'character', 'in', 'the', 'word', '.', 'Thus', ',', 'while', '.', 'is', 'special', ',', '\\\\', '.', 'only', 'matches', 'a', 'period', '.', 'The', 'braced', 'expressions', ',', 'like', '{', '3,5', '}', ',', 'specify', 'the', 'number', 'of', 'repeats', 'of', 'the', 'previous', 'item', '.', 'The', 'pipe', 'character', 'indicates', 'a', 'choice', 'between', 'the', 'material', 'on', 'its', 'left', 'or', 'its', 'right', '.', 'Parentheses', 'indicate', 'the', 'scope', 'of', 'an', 'operator', ':', 'they', 'can', 'be', 'used', 'together', 'with', 'the', 'pipe', '(', 'or', 'disjunction', ')', 'symbol', 'like', 'this', ':', '«', 'w', '(', 'i|e|ai|oo', ')', 't', '»', ',', 'matching', 'wit', ',', 'wet', ',', 'wait', ',', 'and', 'woot', '.', 'It', 'is', 'instructive', 'to', 'see', 'what', 'happens', 'when', 'you', 'omit', 'the', 'parentheses', 'from', 'the', 'last', 'expression', 'above', '.']\n",
      "\n",
      "\n",
      "Tokenization using regular expression\n",
      "['You', 'probably', 'worked', 'out', 'that', 'a', 'backslash', 'means', 'that', 'the', 'following', 'character', 'is', 'deprived', 'of', 'its', 'special', 'powers', 'and', 'must', 'literally', 'match', 'a', 'specific', 'character', 'in', 'the', 'word', '.', 'Thus', ',', 'while', '.', 'is', 'special', ',', '\\\\', '.', 'only', 'matches', 'a', 'period', '.', 'The', 'braced', 'expressions', ',', 'like', '{', '3', ',', '5', '}', ',', 'specify', 'the', 'number', 'of', 'repeats', 'of', 'the', 'previous', 'item', '.', 'The', 'pipe', 'character', 'indicates', 'a', 'choice', 'between', 'the', 'material', 'on', 'its', 'left', 'or', 'its', 'right', '.', 'Parentheses', 'indicate', 'the', 'scope', 'of', 'an', 'operator', ':', 'they', 'can', 'be', 'used', 'together', 'with', 'the', 'pipe', '(', 'or', 'disjunction', ')', 'symbol', 'like', 'this', ':', '«', 'w', '(', 'i', '|', 'e', '|', 'ai', '|', 'oo', ')', 't', '»', ',', 'matching', 'wit', ',', 'wet', ',', 'wait', ',', 'and', 'woot', '.', 'It', 'is', 'instructive', 'to', 'see', 'what', 'happens', 'when', 'you', 'omit', 'the', 'parentheses', 'from', 'the', 'last', 'expression', 'above', '.']\n"
     ]
    }
   ],
   "source": [
    "#Question 1.2\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#read data from Data_1\n",
    "individual_data = open(\"C:/Users/jared/Desktop/Data_1.txt\",\"r\")\n",
    "txt_data = individual_data.read()\n",
    "\n",
    "#tokenization using .split function\n",
    "tokens_split = txt_data.split(\" \")\n",
    "print(\"Tokenization using .split function\")\n",
    "print(tokens_split)\n",
    "\n",
    "#tokenization using nltk\n",
    "tokens_nltk = nltk.tokenize.word_tokenize(txt_data)\n",
    "print(\"\\n\\nTokenization using nltk\")\n",
    "print(tokens_nltk)\n",
    "\n",
    "#tokenization using regular expression\n",
    "regular_ex = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
    "tokens_regular_ex = re.findall(regular_ex,txt_data)\n",
    "print(\"\\n\\nTokenization using regular expression\")\n",
    "print(tokens_regular_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "soviet-following",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Stemming with Porter Stemmer\n",
      "['you', 'probabl', 'work', 'out', 'that', 'a', 'backslash', 'mean', 'that', 'the', 'follow', 'charact', 'is', 'depriv', 'of', 'it', 'special', 'power', 'and', 'must', 'liter', 'match', 'a', 'specif', 'charact', 'in', 'the', 'word', '.', 'thu', ',', 'while', '.', 'is', 'special', ',', '\\\\', '.', 'onli', 'match', 'a', 'period', '.', 'the', 'brace', 'express', ',', 'like', '{', '3,5', '}', ',', 'specifi', 'the', 'number', 'of', 'repeat', 'of', 'the', 'previou', 'item', '.', 'the', 'pipe', 'charact', 'indic', 'a', 'choic', 'between', 'the', 'materi', 'on', 'it', 'left', 'or', 'it', 'right', '.', 'parenthes', 'indic', 'the', 'scope', 'of', 'an', 'oper', ':', 'they', 'can', 'be', 'use', 'togeth', 'with', 'the', 'pipe', '(', 'or', 'disjunct', ')', 'symbol', 'like', 'thi', ':', '«', 'w', '(', 'i|e|ai|oo', ')', 't', '»', ',', 'match', 'wit', ',', 'wet', ',', 'wait', ',', 'and', 'woot', '.', 'It', 'is', 'instruct', 'to', 'see', 'what', 'happen', 'when', 'you', 'omit', 'the', 'parenthes', 'from', 'the', 'last', 'express', 'abov', '.']\n",
      "\n",
      "\n",
      "Word Stemming with Lancaster Stemmer\n",
      "['you', 'prob', 'work', 'out', 'that', 'a', 'backslash', 'mean', 'that', 'the', 'follow', 'charact', 'is', 'depr', 'of', 'it', 'spec', 'pow', 'and', 'must', 'lit', 'match', 'a', 'spec', 'charact', 'in', 'the', 'word', '.', 'thu', ',', 'whil', '.', 'is', 'spec', ',', '\\\\', '.', 'on', 'match', 'a', 'period', '.', 'the', 'brac', 'express', ',', 'lik', '{', '3,5', '}', ',', 'spec', 'the', 'numb', 'of', 'rep', 'of', 'the', 'prevy', 'item', '.', 'the', 'pip', 'charact', 'ind', 'a', 'cho', 'between', 'the', 'mat', 'on', 'it', 'left', 'or', 'it', 'right', '.', 'parenthes', 'ind', 'the', 'scop', 'of', 'an', 'op', ':', 'they', 'can', 'be', 'us', 'togeth', 'with', 'the', 'pip', '(', 'or', 'disjunct', ')', 'symbol', 'lik', 'thi', ':', '«', 'w', '(', 'i|e|ai|oo', ')', 't', '»', ',', 'match', 'wit', ',', 'wet', ',', 'wait', ',', 'and', 'woot', '.', 'it', 'is', 'instruct', 'to', 'see', 'what', 'hap', 'when', 'you', 'omit', 'the', 'parenthes', 'from', 'the', 'last', 'express', 'abov', '.']\n",
      "\n",
      "\n",
      "Word Stemming with Regular Expression Stemmer\n",
      "['You', 'probab', 'work', 'out', 'that', 'a', 'backslash', 'mean', 'that', 'the', 'follow', 'character', 'is', 'depriv', 'of', 'its', 'special', 'power', 'and', 'must', 'literal', 'match', 'a', 'specific', 'character', 'in', 'the', 'word.', 'Thus,', 'whil', '.', 'is', 'special,', '\\\\.', 'on', 'matche', 'a', 'period.', 'The', 'brac', 'expressions,', 'lik', '{3,5},', 'specify', 'the', 'number', 'of', 'repeat', 'of', 'the', 'previou', 'item.', 'The', 'pip', 'character', 'indicate', 'a', 'choic', 'between', 'the', 'material', 'on', 'its', 'left', 'or', 'its', 'right.', 'Parenthese', 'indicat', 'the', 'scop', 'of', 'an', 'operator:', 'they', 'can', 'be', 'us', 'together', 'with', 'the', 'pip', '(or', 'disjunction)', 'symbol', 'lik', 'this:', '«w(i|e|ai|oo)t»,', 'match', 'wit,', 'wet,', 'wait,', 'and', 'woot.', 'It', 'is', 'instructiv', 'to', 'see', 'what', 'happen', 'when', 'you', 'omit', 'the', 'parenthese', 'from', 'the', 'last', 'expression', 'above.']\n"
     ]
    }
   ],
   "source": [
    "#Question 2.2\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#read data from Data_1\n",
    "individual_data = open(\"C:/Users/jared/Desktop/Data_1.txt\",\"r\")\n",
    "txt_data = individual_data.read()\n",
    "tokens = word_tokenize(txt_data)\n",
    "\n",
    "#Porter stemmer\n",
    "porter_stem = PorterStemmer()\n",
    "porter_stem_res = [porter_stem.stem(i) for i in tokens]\n",
    "print(\"Word Stemming with Porter Stemmer\")\n",
    "print(porter_stem_res)\n",
    "\n",
    "#Lancaster stemmer\n",
    "lancaster_stem = LancasterStemmer()\n",
    "lancaster_stem_res = [lancaster_stem.stem(j) for j in tokens]\n",
    "print(\"\\n\\nWord Stemming with Lancaster Stemmer\")\n",
    "print(lancaster_stem_res)\n",
    "\n",
    "#Regular Expression Stemmer\n",
    "regular_ex = RegexpStemmer ('ing$|s$|e$|able$|ed$|ly$', min=4)\n",
    "with open(\"C:/Users/jared/Desktop/Data_1.txt\",\"r\") as txt_data:\n",
    "    for line in txt_data:\n",
    "        regular_ex_sin = []\n",
    "        for regular_plu in line.split():\n",
    "            regular_ex_sin.append(regular_ex.stem(regular_plu))\n",
    "print(\"\\n\\nWord Stemming with Regular Expression Stemmer\")\n",
    "print(regular_ex_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "addressed-cement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter stop wordds and punctuation\n",
      "['You', 'probably', 'worked', 'backslash', 'means', 'following', 'character', 'deprived', 'special', 'powers', 'must', 'literally', 'match', 'specific', 'character', 'word', 'Thus', 'special', 'matches', 'period', 'The', 'braced', 'expressions', 'like', '3,5', 'specify', 'number', 'repeats', 'previous', 'item', 'The', 'pipe', 'character', 'indicates', 'choice', 'material', 'left', 'right', 'Parentheses', 'indicate', 'scope', 'operator', 'used', 'together', 'pipe', 'disjunction', 'symbol', 'like', '«', 'w', 'i|e|ai|oo', '»', 'matching', 'wit', 'wet', 'wait', 'woot', 'It', 'instructive', 'see', 'happens', 'omit', 'parentheses', 'last', 'expression']\n"
     ]
    }
   ],
   "source": [
    "#Question 3.1\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "#read data from Data_1\n",
    "individual_data = open(\"C:/Users/jared/Desktop/Data_1.txt\",\"r\")\n",
    "txt_data = individual_data.read()\n",
    "tokens = word_tokenize(txt_data)\n",
    "\n",
    "#creating stop words & punctuation and report output\n",
    "stopword = set(stopwords.words('english'))\n",
    "filtered_words = [txt for txt in tokens if txt not in stopword and txt not in string.punctuation]\n",
    "print(\"Filter stop words and punctuation\")\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "double-blank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter stop word\n",
      "['out', 'that', 'a', 'that', 'the', 'is', 'of', 'its', 'and', 'a', 'in', 'the', 'while', 'is', 'only', 'a', 'the', 'of', 'of', 'the', 'a', 'between', 'the', 'on', 'its', 'or', 'its', 'the', 'of', 'an', 'they', 'can', 'be', 'with', 'the', 'or', 'this', 't', 'and', 'is', 'to', 'what', 'when', 'you', 'the', 'from', 'the', 'above']\n"
     ]
    }
   ],
   "source": [
    "#Question 3.2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#read data from Data_1\n",
    "individual_data = open(\"C:/Users/jared/Desktop/Data_1.txt\",\"r\")\n",
    "txt_data = individual_data.read()\n",
    "tokens = word_tokenize(txt_data)\n",
    "\n",
    "#creating stop words & punctuation and report output\n",
    "stopword = set(stopwords.words('english'))\n",
    "filtered_words = [txt for txt in tokens if txt in stopword]\n",
    "print(\"Filter stop word\")\n",
    "print(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "vertical-techno",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging using NLTK\n",
      "[('The', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cute', 'NN'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB'), ('.', '.')]\n",
      "\n",
      "\n",
      "POS Tagging using TextBlob\n",
      "[('The', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cute', 'NN'), ('cat', 'NN'), ('and', 'CC'), ('chased', 'VBD'), ('away', 'RB')]\n",
      "\n",
      "POS Tagging using Regular Expression\n",
      "[('The', 'DT'), ('little', 'ADJ'), ('yellow', 'ADJ'), ('dog', 'NN'), ('barked', 'ADV'), ('at', None), ('the', None), ('cute', 'NN'), ('cat', 'NN'), ('and', None), ('chased', 'NN'), ('away', 'ADJ'), ('.', None)]\n"
     ]
    }
   ],
   "source": [
    "#Question 4.1\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#read data from Data_2\n",
    "individual_data = open(\"C:/Users/jared/Desktop/Data_2.txt\",\"r\")\n",
    "txt_data = individual_data.read()\n",
    "tokens = word_tokenize(txt_data)\n",
    "\n",
    "#POS Tagging using NLTK\n",
    "nltk_pos = nltk.pos_tag(tokens)\n",
    "print(\"POS Tagging using NLTK\")\n",
    "print(nltk_pos)\n",
    "\n",
    "#POS Tagging using TextBlob\n",
    "blob_pos = TextBlob(txt_data)\n",
    "blob_pos = blob_pos.tags\n",
    "print(\"\\n\\nPOS Tagging using TextBlob\")\n",
    "print(blob_pos)\n",
    "\n",
    "#POS Tagging using Regular Expression\n",
    "pattern = [\n",
    "    (r'(The)$' , 'DT'),\n",
    "    (r'(little)$' , 'ADJ'),\n",
    "    (r'(yellow)$' , 'ADJ'),\n",
    "    (r'(dog)$' , 'NN'),\n",
    "    (r'(barked)$' , 'ADV'),\n",
    "    (r'(cute)$' , 'NN'),\n",
    "    (r'(cat)$' , 'NN'),\n",
    "    (r'(chased)$' , 'NN'),\n",
    "    (r'(away)$' , 'ADJ'),\n",
    "]\n",
    "regular_ex_tag = nltk.RegexpTagger(pattern)\n",
    "tagger = nltk.tag.sequential.RegexpTagger(pattern)\n",
    "regular_ex_pos = tagger.tag(tokens)\n",
    "print(\"\\nPOS Tagging using Regular Expression\")\n",
    "print(regular_ex_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "hearing-genesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (Nom (Adj little) (Adj yellow) (N dog)))\n",
      "  (VP\n",
      "    (VP\n",
      "      (VP (V barked))\n",
      "      (PP (P at) (NP (Det the) (Nom (Adj cute) (N cat)))))\n",
      "    (CC and)\n",
      "    (VP (V chased) (Adv away))))\n"
     ]
    }
   ],
   "source": [
    "#Question 4.4\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "#read data from Data_2\n",
    "individual_data = open(\"C:/Users/jared/Desktop/Data_2.txt\",\"r\")\n",
    "txt_data = individual_data.read()\n",
    "txt_data = txt_data.lower()\n",
    "tokens = word_tokenize(txt_data)\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det PP | Det Nom\n",
    "VP -> V NP | VP PP | V Adv | VP CC VP | V\n",
    "Nom -> Adj N | Adj Adj N\n",
    "Det -> 'the'\n",
    "N -> 'dog' | 'cat'\n",
    "V -> 'barked' | 'chased'\n",
    "P -> 'at'\n",
    "Adj -> 'little' | 'yellow' | 'cute'\n",
    "Adv -> 'away'\n",
    "CC -> 'and'\n",
    "\"\"\")\n",
    "\n",
    "#removal of punctuation\n",
    "punctuation_rem = list(string.punctuation)\n",
    "result = []\n",
    "for txt in tokens:\n",
    "    if txt not in punctuation_rem:\n",
    "        result.append(txt)\n",
    "        \n",
    "#constructing and drawing parse tree\n",
    "parse_tree = nltk.ChartParser(grammar)\n",
    "for tree_parsing in parse_tree.parse(result):\n",
    "    tree_parsing.draw()\n",
    "    \n",
    "print(tree_parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-columbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
